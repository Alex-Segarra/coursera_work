{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "# Final for IBM's Scalable Machine Learning Using Spark"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id = 'toc'></a>\n\n## Table of Contents:\n\n[1. Loading Libraries and Downloading Spark 2.4.5](#s1)\n\n[2. Loading Data](#s2)\n\n[3. Cleaning Data](#s3)\n\n[4. Correlations](#s4)\n\n[5. Creating Train/Test Split](#s5)\n\n[6. Creating Feature Engineering Pipeline Objects and Convenience Functions](#s6)\n\n[7. Linear Regression](#s7)\n\n[8. Gradient Boosted Tree Regression](#s8)\n\n[9. Logistic Regression Classifier](#s9)\n\n[10. Random Forest Classifier](#s10)\n\n[11. Gradient Boosted Tree Classifier](#s11)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id = s1></a>\n### Loading Libraries and Downloading Spark\n\n[Back to Table of Contents](#toc)"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "from IPython.display import Markdown, display\ndef printmd(string):\n    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n\n\nif ('sc' in locals() or 'sc' in globals()):\n    printmd('Do not run this in a Spark Notebook')\n"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Collecting pyspark==2.4.5\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/5a/271c416c1c2185b6cb0151b29a91fff6fcaed80173c8584ff6d20e46b465/pyspark-2.4.5.tar.gz (217.8MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 217.8MB 142kB/s  eta 0:00:01    |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                  | 95.4MB 9.5MB/s eta 0:00:13\n\u001b[?25hCollecting py4j==0.10.7 (from pyspark==2.4.5)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 37.2MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/bf/db/04/61d66a5939364e756eb1c1be4ec5bdce6e04047fc7929a3c3c\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\nSuccessfully installed py4j-0.10.7 pyspark-2.4.5\n"
                }
            ],
            "source": "# Downloading pyspark\n!pip install pyspark==2.4.5"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "#Loading Spark Libraries\n\ntry:\n    from pyspark import SparkContext, SparkConf\n    from pyspark.sql import SparkSession\nexcept ImportError as e:\n    printmd('If you are getting this error, restart your Kernel')"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "# Setting SparkConf to Local Machine\nsc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n\n#Creating Sparksql object\nspark = SparkSession.builder.getOrCreate()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id = 's2'></a>\n### Loading Data\n\n[Back to Table of Contents](#toc)"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2020-05-05 20:27:48--  http://max-training-data.s3-api.us-geo.objectstorage.softlayer.net/noaa-weather/jfk_weather.tar.gz\nResolving max-training-data.s3-api.us-geo.objectstorage.softlayer.net (max-training-data.s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\nConnecting to max-training-data.s3-api.us-geo.objectstorage.softlayer.net (max-training-data.s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2575759 (2.5M) [application/x-tar]\nSaving to: \u2018jfk_weather.tar.gz\u2019\n\n100%[======================================>] 2,575,759   --.-K/s   in 0.04s   \n\n2020-05-05 20:27:49 (68.3 MB/s) - \u2018jfk_weather.tar.gz\u2019 saved [2575759/2575759]\n\n./._jfk_weather.csv\njfk_weather.csv\n"
                }
            ],
            "source": "## Getting Data\n# delete files from previous runs\n!rm -f jfk_weather*\n\n# download the file containing the data in CSV format\n!wget http://max-training-data.s3-api.us-geo.objectstorage.softlayer.net/noaa-weather/jfk_weather.tar.gz\n\n# extract the data\n!tar xvfz jfk_weather.tar.gz\n    \n# create a dataframe out of it by using the first row as field names and trying to infer a schema based on contents\ndf = spark.read.option(\"header\", \"true\").option(\"inferSchema\",\"true\").csv('jfk_weather.csv')\n\n# register a corresponding query table\ndf.createOrReplaceTempView('df')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id= 's3'></a>\n## Data Cleaning\n[Back to Table of Contents](#toc)"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": "import random\nrandom.seed(42)\n\nfrom pyspark.sql.functions import translate, col\n\n# Cleaning trailing special characters\ndf_cleaned = df \\\n    .withColumn(\"HOURLYWindSpeed\", df.HOURLYWindSpeed.cast('double')) \\\n    .withColumn(\"HOURLYWindDirection\", df.HOURLYWindDirection.cast('double')) \\\n    .withColumn(\"HOURLYStationPressure\", translate(col(\"HOURLYStationPressure\"), \"s,\", \"\")) \\\n    .withColumn(\"HOURLYPrecip\", translate(col(\"HOURLYPrecip\"), \"s,\", \"\")) \\\n    .withColumn(\"HOURLYRelativeHumidity\", translate(col(\"HOURLYRelativeHumidity\"), \"*\", \"\")) \\\n    .withColumn(\"HOURLYDRYBULBTEMPC\", translate(col(\"HOURLYDRYBULBTEMPC\"), \"*\", \"\")) \\\n\n# Casting as Doubles\ndf_cleaned =   df_cleaned \\\n                    .withColumn(\"HOURLYStationPressure\", df_cleaned.HOURLYStationPressure.cast('double')) \\\n                    .withColumn(\"HOURLYPrecip\", df_cleaned.HOURLYPrecip.cast('double')) \\\n                    .withColumn(\"HOURLYRelativeHumidity\", df_cleaned.HOURLYRelativeHumidity.cast('double')) \\\n                    .withColumn(\"HOURLYDRYBULBTEMPC\", df_cleaned.HOURLYDRYBULBTEMPC.cast('double')) \\\n\ndf_filtered = df_cleaned.filter(\"\"\"\n    HOURLYWindSpeed <> 0\n    and HOURLYWindSpeed IS NOT NULL\n    and HOURLYWindDirection IS NOT NULL\n    and HOURLYStationPressure IS NOT NULL\n    and HOURLYPressureTendency IS NOT NULL\n    and HOURLYPrecip IS NOT NULL\n    and HOURLYRelativeHumidity IS NOT NULL\n    and HOURLYDRYBULBTEMPC IS NOT NULL\n\"\"\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id = 's4'></a>\n## Correlations\n\n[Back to Table of Contents](#toc)"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([[ 1.        , -0.01324305],\n       [-0.01324305,  1.        ]])"
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "from pyspark.ml.feature import VectorAssembler\nimport seaborn as sns\nvectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYPressureTendency\"],\n                                  outputCol=\"features\")\n\ndf_pipeline = vectorAssembler.transform(df_filtered)\n\nfrom pyspark.ml.stat import Correlation\n\ncorr = Correlation.corr(df_pipeline,\"features\").head()[0].toArray()\ncorr"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id = 's5'></a>\n## Creating Train/Test Split\n\n[Back to Table of Contents](#toc)"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": "splits = df_filtered.randomSplit([.8,.2])\n\ndf_train = splits[0]\ndf_test = splits[1]"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id ='s6'></a>\n## Creating Feature Engineering Pipeline Objects / Convenience Functions\n\n[Back to Table of Contents](#toc)"
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\nfrom pyspark.ml import Pipeline\n\ndef create_ml_pipeline(vector_assembler = '' ,normalizer_ = False, onehotencoder = '', bucketizer = '', model = ''):\n\n    if vector_assembler:\n        vectorassembler = VectorAssembler(inputCols = vector_assembler ,outputCol=\"features\")  \n        print('Vectors Assembled\\n')\n        if normalizer_ == True:\n            normalizer_ = Normalizer(inputCol = \"features\", outputCol=\"features_norm\", p=1.0)\n            print('Normalized\\n')\n           # if model:\n            #    print('Changed the features column name to \"features_norm\"\\n')\n             #   model.featuresCol = 'features_norm'\n    if bucketizer:\n        from pyspark.ml.feature import Bucketizer, OneHotEncoderEstimator\n        bucketizer = Bucketizer(splits=[ 0, 180, float('Inf') ],inputCol=bucketizer, outputCol=bucketizer+\"Bucketized\")\n        print('Bucketized!')\n        if onehotencoder:\n            onehotencoder = OneHotEncoderEstimator(inputCol = bucketizer+\"Bucketized\" , outputCol = onehotencoder+'OHE')\n            print('...And Hot Encoded\\n')\n    if onehotencoder:\n            onehotencoder = OneHotEncoderEstimator(inputCol = onehotencoder , outputCol = onehotencoder+'OHE')\n            print('One Hot Encoded\\n')\n    \n    if model:\n        print('Your model is ready to be put in the pipe!\\n')\n    \n    pipe = [x for x in [vectorassembler,normalizer_,bucketizer,onehotencoder,model] if x]\n    \n    print('Here is the Pipeline:')\n    [print(p) for p in pipe]\n    \n    pipeline = Pipeline(stages = pipe)\n    \n    return pipeline\n\n\n"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": "def regression_metrics(prediction):\n    from pyspark.ml.evaluation import RegressionEvaluator\n    evaluator = RegressionEvaluator(\n    labelCol=\"HOURLYWindSpeed\", predictionCol=\"prediction\", metricName=\"rmse\")\n    rmse = evaluator.evaluate(prediction)\n    print(\"\\nRMSE on test data = %g\" % rmse)"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": "def classification_metrics(prediction):\n    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n    mcEval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\") .setPredictionCol(\"prediction\").setLabelCol(\"HOURLYWindDirectionBucketized\")\n    accuracy = mcEval.evaluate(prediction)\n    print(\"Accuracy on test data = %g\" % accuracy)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id = 's7'></a>\n## Linear Regression\n[Back to Table of Contents](#toc)"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Vectors Assembled\n\nNormalized\n\nYour model is ready to be put in the pipe!\n\nHere is the Pipeline:\nVectorAssembler_488261704753\nNormalizer_2d354caf5ecb\nLinearRegression_45c7bb426ac9\n\nRMSE on test data = 5.35498\n"
                }
            ],
            "source": "from pyspark.ml.regression import LinearRegression\n\nvec = [\"HOURLYWindDirection\",\"ELEVATION\",\"HOURLYStationPressure\"]\nlr = LinearRegression(labelCol=\"HOURLYWindSpeed\", featuresCol='features', maxIter=100, regParam=0.0, elasticNetParam=0.0) \n\n#LR1\npipeline = create_ml_pipeline(vector_assembler= vec , normalizer_=True, model = lr)\n\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)\nregression_metrics(prediction)"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Vectors Assembled\n\nNormalized\n\nYour model is ready to be put in the pipe!\n\nHere is the Pipeline:\nVectorAssembler_48d4a38c95db\nNormalizer_b243060d6bf6\nLinearRegression_0efc1e827425\n\nRMSE on test data = 5.58542\n"
                }
            ],
            "source": "#LR2 - with normalized data\n\nvec = [\"HOURLYWindDirection\",\"ELEVATION\",\"HOURLYStationPressure\"]\nlr = LinearRegression(labelCol=\"HOURLYWindSpeed\", featuresCol='features_norm', maxIter=100, regParam=0.0, elasticNetParam=0.0) \n\npipeline = create_ml_pipeline(vector_assembler= vec , normalizer_=True, model = lr)\n\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)\nregression_metrics(prediction)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id = 's8'></a>\n## Gradient Boosted Tree\n[Back to Table of Contents](#toc)"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Vectors Assembled\n\nNormalized\n\nYour model is ready to be put in the pipe!\n\nHere is the Pipeline:\nVectorAssembler_e36364e35ce7\nNormalizer_a107557c3595\nGBTRegressor_98b5c0d4fa7a\n\nRMSE on test data = 5.1381\n"
                }
            ],
            "source": "from pyspark.ml.regression import GBTRegressor\n\ngbt = GBTRegressor(labelCol=\"HOURLYWindSpeed\", maxIter=100)\nvec = [\"HOURLYWindDirection\",\"ELEVATION\",\"HOURLYStationPressure\"]\n\npipeline = create_ml_pipeline(vector_assembler= vec , normalizer_=True, model = gbt)\n\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)\nregression_metrics(prediction)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id = 's9'></a>\n## Logistic Regression\n[Back to Table of Contents](#toc)"
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Vectors Assembled\n\nNormalized\n\nBucketized!\nYour model is ready to be put in the pipe!\n\nHere is the Pipeline:\nVectorAssembler_f638c2f61704\nNormalizer_b711cb005012\nBucketizer_f01e531de411\nLogisticRegression_618b39389237\nAccuracy on test data = 0.688165\n"
                }
            ],
            "source": "#LGReg1\n\nfrom pyspark.ml.classification import LogisticRegression\n\nvec = [\"HOURLYWindSpeed\",\"HOURLYDRYBULBTEMPC\"]\nlr =  LogisticRegression(labelCol=\"HOURLYWindDirectionBucketized\", maxIter=10)\npipeline = create_ml_pipeline(vector_assembler= vec , bucketizer='HOURLYWindDirection', normalizer_=True, model = lr)\n\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)\nclassification_metrics(prediction)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id = 's10'></a>\n## Random Forest\n[Back to Table of Contents](#toc)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.ml.classification import RandomForestClassifier\nrf = RandomForestClassifier(labelCol=\"HOURLYWindDirectionBucketized\", numTrees=10)\npipeline = create_ml_pipeline(vector_assembler= vec , bucketizer='HOURLYWindDirection', normalizer_=True, model = rf)\n\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)\nclassification_metrics(prediction)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id = 's11'></a>\n## Gradient Boosted Tree Classifier\n[Back to Table of Contents](#toc)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Vectors Assembled\n\nNormalized\n\nBucketized!\nYour model is ready to be put in the pipe!\n\nHere is the Pipeline:\nVectorAssembler_25b92d73de42\nNormalizer_fefa51e8a517\nBucketizer_fc8aeae1efb5\nGBTClassifier_79b34ff73182\n"
                }
            ],
            "source": "from pyspark.ml.classification import GBTClassifier\ngbt = GBTClassifier(labelCol=\"HOURLYWindDirectionBucketized\", maxIter=100)\n\npipeline = create_ml_pipeline(vector_assembler= vec , bucketizer='HOURLYWindDirection', normalizer_=True, model = gbt)\n\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)\nclassification_metrics(prediction)\n"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}